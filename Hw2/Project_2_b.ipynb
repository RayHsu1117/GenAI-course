{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEjfoWqvxgn1"
      },
      "source": [
        "# T5 英文文本生成\n",
        "## 教學目標\n",
        "使用T5模型根據英文關鍵字生成完整的句子\n",
        "\n",
        "## 適用對象\n",
        " - 已對python的基本語法和有一定瞭解和掌握程度\n",
        " - 對深度學習的基本概念有初步的認識\n",
        "\n",
        "## 執行方法\n",
        "在 Jupyter notebook 中，選取想要執行的區塊後，使用以下其中一種方法執行\n",
        " - 上方工具列中，按下 Cell < Run Cells 執行\n",
        " - 使用快捷鍵 Shift + Enter 執行\n",
        "\n",
        "## 大綱\n",
        " - [安裝套件](#安裝套件)\n",
        " - [載入T5模型](#載入T5模型)\n",
        " - [資料處理](#資料處理)\n",
        " - [超參數](#超參數)\n",
        " - [訓練](#訓練)\n",
        " - [驗證](#驗證)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGHejOlhxgn2"
      },
      "source": [
        "## 安裝套件\n",
        " - transformers (4.37.0) huggingface讀取模型的套件\n",
        " - datasets (2.16.1) huggingface讀取資料集的套件\n",
        " - torcheval (0.0.7) 各種評價標準"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bANlfRj8xgn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6464faa-c63a-4931-b035-98c41f2b3b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.11.0)\n",
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.10/dist-packages (0.5.0.post2)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install torcheval\n",
        "! pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fmaIzlYkxgn3"
      },
      "outputs": [],
      "source": [
        "import transformers as T\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from ignite.metrics import Rouge\n",
        "import re\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, RobertaTokenizer,BertTokenizerFast\n"
      ],
      "metadata": {
        "id": "VNSJioPHNfl0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwfsJbvyxgn3"
      },
      "source": [
        "## 載入T5模型\n",
        " - 使用huggingface裝載模型的架構、參數和tokenizer\n",
        " - 保存在路徑./cache/中\n",
        " - 用.to(device)把模型裝載入訓練設備(GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "978amJhFxgn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8fb3e31-2a0f-4ebd-9ac5-d93baf2df9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "t5_model = T.T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", cache_dir=\"./cache/\").to(device)\n",
        "t5_tokenizer = T.T5Tokenizer.from_pretrained(\"google/flan-t5-base\", cache_dir=\"./cache/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import jieba"
      ],
      "metadata": {
        "id": "fUvaSwGFlnTh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
      ],
      "metadata": {
        "id": "3uEFFiR8NVpz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_fast = BertTokenizerFast.from_pretrained(\"bert-base-chinese\",add_special_tokens=False,do_lower_case=True)"
      ],
      "metadata": {
        "id": "SWJx-NV5wGxe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_model = T.GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=\"./cache/\").to(device)\n",
        "gpt2_tokenizer = T.GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=\"./cache/\")\n",
        "# gpt2_model = T.GPT2ForSequenceClassification.from_pretrained(\"gpt2\", cache_dir=\"./cache/\").to(device)\n",
        "# gpt2_tokenizer = T.GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=\"./cache/\")\n",
        "gpt2_tokenizer.padding_side = \"left\"\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "# gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ],
      "metadata": {
        "id": "Zs040j49wM_r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast,AutoModelForCausalLM\n",
        "\n",
        "tokenizer_3 = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
        "model_3 = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese').to(device)"
      ],
      "metadata": {
        "id": "xl3y7R-oPel7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARnj-n9Kxgn3"
      },
      "source": [
        "## 資料處理\n",
        " - 使用 torch.utils.data 中的 Dataset 和 Dataloader 成批次地讀取和預處理資料\n",
        " - 使用“/”將每個輸入的關鍵字和每個輸出鏈接起來"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aHXXUKHRxgn3"
      },
      "outputs": [],
      "source": [
        "# def get_tensor(sample):\n",
        "#     # 將模型的輸入和ground truth打包成Tensor\n",
        "#     # t5 tokenizer不能處理中文，所以換成bert的試試看\n",
        "#     # model_inputs = t5_tokenizer.batch_encode_plus([each[\"text\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#     # model_outputs = t5_tokenizer.batch_encode_plus([each[\"summary\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#     # bert\n",
        "#     # model_inputs = bert_tokenizer.batch_encode_plus([each[\"text\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#     # model_outputs = bert_tokenizer.batch_encode_plus([each[\"summary\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#     # bertfast\n",
        "#     model_inputs = bert_tokenizer_fast.batch_encode_plus([each[\"text\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\",return_offsets_mapping=True,add_special_tokens=False)\n",
        "#     model_outputs = bert_tokenizer_fast.batch_encode_plus([each[\"summary\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\",return_offsets_mapping=True,add_special_tokens=False)\n",
        "#     return model_inputs[\"input_ids\"].to(device), model_outputs[\"input_ids\"].to(device)\n",
        "\n",
        "# class CommonGenDataset(Dataset):\n",
        "#     def __init__(self, split=\"train\") -> None:\n",
        "#         super().__init__()\n",
        "#         assert split in [\"train\", \"validation\", \"test\"]\n",
        "#         # data_df = load_dataset(\"allenai/common_gen\", split=split, cache_dir=\"./cache/\").to_pandas().groupby(\"concept_set_idx\")\n",
        "\n",
        "#         data_df = load_dataset(\"hugcyp/LCSTS\", split=split, cache_dir=\"./cache/\").to_pandas()\n",
        "#         self.data = []\n",
        "#         # for each in data_df:\n",
        "#         #     targets = \"/ \".join([s+\".\" if not s.endswith(\".\") else s for s in each[1].target.to_list()])\n",
        "#         #     concepts = \", \".join(each[1].concepts.to_list()[0])\n",
        "#         #     self.data.append({\"concepts\": concepts, \"targets\": targets})\n",
        "#         for num in range(0,len(data_df)):\n",
        "#           self.data.append({\"summary\":data_df['summary'][num],\"text\":data_df['text'][num]})\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.data[index]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_gpt2(sample):\n",
        "    input_ids = gpt2_tokenizer.batch_encode_plus([each[\"text\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "    labels = gpt2_tokenizer.batch_encode_plus([each[\"summary\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "    return input_ids, labels\n",
        "class CommonGenDataset_gpt2(Dataset):\n",
        "    def __init__(self, split=\"train\") -> None:\n",
        "        super().__init__()\n",
        "        assert split in [\"train\", \"validation\", \"test\"]\n",
        "        data_df = load_dataset(\"hugcyp/LCSTS\", split=split, cache_dir=\"./cache/\").to_pandas()\n",
        "        self.data = [{\"summary\":data_df['summary'][num],\"text\":data_df['text'][num]} for num in range(len(data_df))]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "g4cI7cxexeKJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kobvz1xgxgn3"
      },
      "source": [
        "## 超參數\n",
        " - 學習率 (learning rate): 1e-5\n",
        " - 訓練輪數 (epochs): 3\n",
        " - 優化器 (optimizer): AdamW\n",
        " - 批次大小 (batch size): 8\n",
        " - 評量指標 (evaluation matrics)Rouge-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dWTegnXXxgn3"
      },
      "outputs": [],
      "source": [
        "# lr = 1e-5\n",
        "# epochs = 1\n",
        "# optimizer = AdamW(t5_model.parameters(), lr = 1e-5)\n",
        "# train_batch_size = 8\n",
        "# validation_batch_size = 8\n",
        "# common_gen_train = DataLoader(CommonGenDataset(split=\"train\").data[:200], collate_fn=get_tensor, batch_size=train_batch_size, shuffle=True)\n",
        "# common_gen_validation = DataLoader(CommonGenDataset(split=\"validation\").data[:200], collate_fn=get_tensor, batch_size=validation_batch_size, shuffle=False)\n",
        "# rouge = Rouge(variants=[\"L\", 2], multiref=\"best\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-5\n",
        "epochs = 1\n",
        "optimizer = AdamW(gpt2_model.parameters(), lr=lr)\n",
        "train_batch_size = 8\n",
        "validation_batch_size = 8\n",
        "common_gen_train_gpt2 = DataLoader(CommonGenDataset_gpt2(split=\"train\").data[:200], collate_fn=get_tensor_gpt2, batch_size=train_batch_size, shuffle=True)\n",
        "common_gen_validation_gpt2 = DataLoader(CommonGenDataset_gpt2(split=\"validation\").data[:200], collate_fn=get_tensor_gpt2, batch_size=validation_batch_size, shuffle=False)\n",
        "rouge = Rouge(variants=[\"L\", 2], multiref=\"best\")"
      ],
      "metadata": {
        "id": "Q__5BbBmxuUA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP0ZFCSvxgn4"
      },
      "source": [
        "## 驗證\n",
        "驗證程式\n",
        " - 將驗證資料輸入模型，用Rouge-2評價輸出的效果\n",
        " - Rouge的使用方法參考 https://pytorch.org/ignite/generated/ignite.metrics.Rouge.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AyaTmD-Hxgn4"
      },
      "outputs": [],
      "source": [
        "# def evaluate(model):\n",
        "#     pbar = tqdm(common_gen_validation)\n",
        "#     pbar.set_description(f\"Evaluating\")\n",
        "\n",
        "#     for inputs, targets in pbar:\n",
        "#         # output = [re.split(r\"[/]\", each.replace(\"<pad>\", \"\")) for each in t5_tokenizer.batch_decode(model.generate(inputs, max_length=50))]\n",
        "#         # targets = [re.split(r\"[/]\", each.replace(\"<pad>\", \"\")) for each in t5_tokenizer.batch_decode(targets)]\n",
        "#         output = [re.split(r\"[/]\", each.replace(\"<pad>\", \"\")) for each in bert_tokenizer.batch_decode(model.generate(inputs, max_length=50))]\n",
        "#         targets = [re.split(r\"[/]\", each.replace(\"<pad>\", \"\")) for each in bert_tokenizer.batch_decode(targets)]\n",
        "#         for i in range(len(output)):\n",
        "#             sentences = [s.replace('.', ' .').split() for s in output[i]]\n",
        "#             ground_thruths = [t.replace('.', ' .').split() for t in targets[i]]\n",
        "#             for s in sentences:\n",
        "#                 rouge.update(([s], [ground_thruths]))\n",
        "#     return rouge.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_gpt2(model):\n",
        "    pbar = tqdm(common_gen_validation_gpt2)\n",
        "    pbar.set_description(f\"Evaluating\")\n",
        "\n",
        "    for inputs, targets in pbar:\n",
        "        output = gpt2_model.generate(inputs, max_length=50, num_return_sequences=1, temperature=1.0)\n",
        "        output = [gpt2_tokenizer.decode(ids, skip_special_tokens=True) for ids in output]\n",
        "        targets = [gpt2_tokenizer.decode(ids, skip_special_tokens=True) for ids in targets]\n",
        "        for i in range(len(output)):\n",
        "            rouge.update([(output[i].split(), targets[i].split())])\n",
        "    return rouge.compute()"
      ],
      "metadata": {
        "id": "0c5e3Wz-yEog"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGZgD2IBxgn4"
      },
      "source": [
        "## 訓練\n",
        " - 將資料成批次輸入T5模型，並獲取其損失函數數值，隨後計算梯度優化\n",
        " - tqdm用來顯示模型的訓練進度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MjPwHOCmxgn4"
      },
      "outputs": [],
      "source": [
        "# for ep in range(epochs):\n",
        "#     pbar = tqdm(common_gen_train)\n",
        "#     pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
        "#     for inputs, targets in pbar:\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = t5_model(input_ids=inputs, labels=targets).loss\n",
        "#         loss.backward()\n",
        "\n",
        "#         optimizer.step()\n",
        "#         pbar.set_postfix(loss = loss.item())\n",
        "#     # torch.save(t5_model, f'./saved_models/ep{ep}.mod')\n",
        "#     print(f\"Rouge-2 score on epoch {ep}:\", evaluate(t5_model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(epochs):\n",
        "    pbar = tqdm(common_gen_train_gpt2)\n",
        "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
        "    for inputs, targets in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        # print()\n",
        "        # print(inputs)\n",
        "        # print(targets)\n",
        "        padding_needed = inputs.size(1) - targets.size(1)\n",
        "\n",
        "        # 定义填充参数\n",
        "        padding = (0, padding_needed)\n",
        "\n",
        "        # 在张量后面填充\n",
        "        padded_targets = torch.nn.functional.pad(targets, padding, value=0)\n",
        "\n",
        "        pad_token_id = gpt2_tokenizer.pad_token_id\n",
        "        inputs[inputs == pad_token_id] = gpt2_tokenizer.eos_token_id  # 將填充標記 ID 替換為 EOS（結束）標記 ID\n",
        "\n",
        "        # # 創建注意力遮罩\n",
        "        attention_mask = inputs.ne(gpt2_tokenizer.pad_token_id)\n",
        "        # output = gpt2_model.generate(inputs, max_length=inputs.size(1)+1, num_return_sequences=1, temperature=1.0)\n",
        "        loss = gpt2_model(input_ids=inputs, labels=padded_targets,attention_mask=attention_mask).loss\n",
        "        # loss = gpt2_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "    print(f\"Rouge-2 score on epoch {ep}:\", evaluate_gpt2(gpt2_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeAujs6uyOwG",
        "outputId": "24a1b9af-ce7c-4cc1-f2ef-a6d19d8de928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [1/1]:   4%|▍         | 1/25 [00:49<19:52, 49.70s/it, loss=10.7]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Rouge-2 score on epoch {ep}:\", evaluate(t5_model))"
      ],
      "metadata": {
        "id": "5FSkY1navC4M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}