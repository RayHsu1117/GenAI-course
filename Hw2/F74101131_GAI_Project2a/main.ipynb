{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2V49SGussdG"
      },
      "source": [
        "# char-RNN-文本生成\n",
        "## 教學目標\n",
        "使用 RNN 弄出一個基本的生成文字模型，幫助初學者上手 RNN\n",
        "\n",
        "## 適用對象\n",
        "適用於已經學過 PyTorch 基本語法的人\n",
        "\n",
        "## 執行方法\n",
        "在 Jupyter notebook 中，選取想要執行的區塊後，使用以下其中一種方法執行\n",
        "\n",
        "- 上方工具列中，按下 Cell < Run Cells 執行\n",
        "- 使用快捷鍵 Shift + Enter 執行\n",
        "\n",
        "## 大綱\n",
        "- [載入資料](#載入資料)\n",
        "- [前處理](#前處理)\n",
        "- [建立字典](#建立字典)\n",
        "- [超參數](#超參數)\n",
        "- [資料分批](#資料分批)\n",
        "- [模型設計](#模型設計)\n",
        "- [訓練](#訓練)\n",
        "- [生成](#生成)\n",
        "\n",
        "## 檔案來源\n",
        "- [Kaggle HC 新聞資料集](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)\n",
        "- 下載後請放到路徑 `專案資料夾/data/old-newspaper.tsv`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0hNnpWiYhxA",
        "outputId": "d9436728-b661-464e-ece2-ea3acfd0afb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paK_oRr5tNl4",
        "outputId": "6b152fe1-5418-48b5-fa39-8465dba2af00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gw8STzsduw_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d15a5b-1ec9-4f82-f771-c47853183754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.7-cp310-cp310-manylinux1_x86_64.whl (779 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.8/779.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install opencc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fVkWj8xYssdI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.utils.rnn\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import opencc\n",
        "\n",
        "data_path = './gdrive/MyDrive/ikm_lab/GAI/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd7qSt-0ssdI"
      },
      "source": [
        "# 載入資料\n",
        "- 請務必先[下載](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)資料後將資料放置到 `data` 資料夾之下\n",
        "- `tsv` 檔案類似 `csv`，只是用 `\\t` 做分隔符號\n",
        "- 資料內容包含\n",
        "\n",
        "|欄位|意義|資料型態|\n",
        "|-|-|-|\n",
        "|`Language`|語系|文字（類別）|\n",
        "|`Source`|新聞來源|文字|\n",
        "|`Date`|時間|文字|\n",
        "|`Text`|文字內容|文字|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "prwGlvaJssdJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_path + '/arithmetic.csv'))\n",
        "# 看一下前幾筆資料是什麼樣子\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jwe0n0ZssdJ"
      },
      "source": [
        "# 前處理\n",
        "- 訓練目標為生成繁體中文字\n",
        "    - 所以只考量繁體中文的資料\n",
        "    - 類別為 `Chinese (Traditional)`\n",
        "    - 共約 333735 筆\n",
        "- 資料長度不一\n",
        "    - 畫出長度分佈圖\n",
        "    - 計算長度四分位數、最小值、最大值\n",
        "    - 為了方便訓練，只考慮長度介於 60~200 的新聞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZIO7XWTgssdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08219ae8-ef53-49ce-fc66-1c238df9ffd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2632500, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# df[df['Language'] == 'Chinese (Traditional)'].shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c8e0_N3BssdK"
      },
      "outputs": [],
      "source": [
        "# 只取前7000筆，因爲原資料量太大了，不方便演示\n",
        "# df = df[df['Language'] == 'Chinese (Traditional)'].iloc[:7000]\n",
        "# df['src'].iloc[:]\n",
        "\n",
        "# df = df.iloc[:26325]\n",
        "df['src'] = df['src'].astype(str)\n",
        "df['tgt'] = df['tgt'].astype(str)\n",
        "df['combined'] = df['src'].str.cat(df['tgt'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhROnO8llIKN",
        "outputId": "414d0c13-4d0a-4397-9f2a-86eb20e05025"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2632500, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY2NI3XissdK"
      },
      "source": [
        "# 建立字典\n",
        "- 無法直接利用純文字進行計算\n",
        "- 將所有文字轉換成數字\n",
        "- 字典大小約為 `7000`\n",
        "- 特殊字\n",
        "    - '&lt;pad&gt;'\n",
        "        - 每個 batch 所包含的句子長度不同\n",
        "        - 將長度使用 '&lt;pad&gt;' 補成 batch 中最大值者\n",
        "    - '&lt;eos&gt;'\n",
        "        - 指定生成的結尾\n",
        "        - 沒有 '&lt;eos&gt;' 會不知道何時停止生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Em6Hod-AssdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb4b409-ed42-4f3a-d6f3-8884f833ad8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "字典大小: 18\n"
          ]
        }
      ],
      "source": [
        "# 一個dict把中文字符轉化成id\n",
        "char_to_id = {}\n",
        "# 把id轉回中文字符\n",
        "id_to_char = {}\n",
        "\n",
        "# 有一些必須要用的special token先添加進來(一般用來做padding的token的id是0)\n",
        "char_to_id['<pad>'] = 0\n",
        "char_to_id['<eos>'] = 1\n",
        "id_to_char[0] = '<pad>'\n",
        "id_to_char[1] = '<eos>'\n",
        "\n",
        "\n",
        "# 把所有資料集中出現的token都記錄到dict中\n",
        "for char in set(df['combined'].str.cat()):\n",
        "    ch_id = len(char_to_id)\n",
        "    char_to_id[char] = ch_id\n",
        "    id_to_char[ch_id] = char\n",
        "\n",
        "vocab_size = len(char_to_id)\n",
        "print('字典大小: {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yJ6J6la1ssdL"
      },
      "outputs": [],
      "source": [
        "# # 把資料集的所有資料都變成id\n",
        "df['char_id_list'] = df['combined'].apply(lambda text: [char_to_id[char] for char in list(text)] + [char_to_id['<eos>']])\n",
        "df['ans_id_list'] = df['tgt'].apply(lambda text: [char_to_id[char] for char in list(text)] + [char_to_id['<eos>']])\n",
        "# df[['src','tgt','combined','char_id_list','ans_id_list']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bJ4d_hwpssdK"
      },
      "outputs": [],
      "source": [
        "brackets_df = pd.DataFrame(columns=['src', 'tgt','combined','char_id_list','ans_id_list'])\n",
        "no_brackets_df = pd.DataFrame(columns=['src', 'tgt','combined','char_id_list','ans_id_list'])\n",
        "\n",
        "positive_df = pd.DataFrame(columns=['src', 'tgt','combined','char_id_list','ans_id_list'])\n",
        "negative_df = pd.DataFrame(columns=['src', 'tgt','combined','char_id_list','ans_id_list'])\n",
        "total_df = pd.DataFrame(columns=['src', 'tgt','combined','char_id_list','ans_id_list'])\n",
        "\n",
        "\n",
        "first_df = df.iloc[:10000]\n",
        "last_df = df.iloc[2622500:]\n",
        "\n",
        "for src,tgt,combined,char_id,ans_id in zip(first_df['src'],first_df['tgt'],first_df['combined'],first_df['char_id_list'],first_df['ans_id_list']):\n",
        "\n",
        "  total_df.loc[len(total_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  if '(' in src:\n",
        "    brackets_df.loc[len(brackets_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  else :\n",
        "    no_brackets_df.loc[len(no_brackets_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  if '-' in tgt:\n",
        "    negative_df.loc[len(negative_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  else  :\n",
        "    positive_df.loc[len(positive_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "for src,tgt,combined,char_id,ans_id in zip(last_df['src'],last_df['tgt'],last_df['combined'],last_df['char_id_list'],last_df['ans_id_list']):\n",
        "  total_df.loc[len(total_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  if '(' in src:\n",
        "    brackets_df.loc[len(brackets_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  else :\n",
        "    no_brackets_df.loc[len(no_brackets_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  if '-' in tgt:\n",
        "    negative_df.loc[len(negative_df)] = [src,tgt,combined,char_id,ans_id]\n",
        "  else  :\n",
        "    positive_df.loc[len(positive_df)] = [src,tgt,combined,char_id,ans_id]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q8LAMUBssdL"
      },
      "source": [
        "# 超參數\n",
        "\n",
        "|超參數|意義|數值|\n",
        "|-|-|-|\n",
        "|`batch_size`|單一 batch 的資料數|64|\n",
        "|`epochs`|總共要訓練幾個 epoch|10|\n",
        "|`embed_dim`|文字的 embedding 維度|50|\n",
        "|`hidden_dim`|LSTM 中每個時間的 hidden state 維度|50|\n",
        "|`lr`|Learning Rate|0.001|\n",
        "|`grad_clip`|為了避免 RNN 出現梯度爆炸問題，將梯度限制範圍|1|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pgncDLJCssdL"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "epochs = 3\n",
        "embed_dim = 256\n",
        "hidden_dim = 256\n",
        "lr = 0.001\n",
        "grad_clip = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r15x449HssdM"
      },
      "source": [
        "# 資料分批\n",
        "- 使用 `torch.utils.data.Dataset` 建立資料產生的工具 `dataset`\n",
        "- 再使用 `torch.utils.data.DataLoader` 對資料集 `dataset` 隨機抽樣並作為一個 batch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 這裏的dataset是文本生成的dataset，輸入和輸出的資料都是文章\n",
        "# 舉個例子，現在的狀況是：\n",
        "# input:  A B C D E F\n",
        "# output: B C D E F <eos>\n",
        "# 而對於加減法的任務：\n",
        "# input:  1 + 2 + 3 = 6\n",
        "# output: / / / / / 6 <eos>\n",
        "# /的部分都不用算loss，主要是預測=的後面，這裏的答案是6，所以output是6 <eos>\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, equation, answer):\n",
        "        self.equation = equation\n",
        "        self.answer = answer\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # input:  A B C D E F\n",
        "        # output: B C D E F <eos>\n",
        "        x = self.equation.iloc[index][:]\n",
        "        y = self.answer.iloc[index][:]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.equation)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_x = [torch.tensor(data[0]) for data in batch] # list[torch.tensor]\n",
        "    batch_y = [torch.tensor(data[1]) for data in batch] # list[torch.tensor]\n",
        "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
        "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
        "\n",
        "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(batch_x,\n",
        "                            batch_first=True, # shape=(batch_size, seq_len)\n",
        "                            padding_value=char_to_id['<pad>'])\n",
        "\n",
        "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(batch_y,\n",
        "                            batch_first=True, # shape=(batch_size, seq_len)\n",
        "                            padding_value=char_to_id['<pad>'])\n",
        "\n",
        "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens\n"
      ],
      "metadata": {
        "id": "CU-qLJvWJ24o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Rd4bWLP1ssdM"
      },
      "outputs": [],
      "source": [
        "math_dataset = Dataset(df['char_id_list'].iloc[263000:283000],df['ans_id_list'].iloc[263000:283000])\n",
        "validation_dataset = Dataset(df['char_id_list'].iloc[226364:226428],df['ans_id_list'].iloc[226364:226428])\n",
        "bracket_dataset = Dataset(brackets_df['char_id_list'],brackets_df['ans_id_list'])\n",
        "no_brackets_dataset = Dataset(no_brackets_df['char_id_list'],no_brackets_df['ans_id_list'])\n",
        "positive_dataset = Dataset(positive_df['char_id_list'],positive_df['ans_id_list'])\n",
        "negative_dataset = Dataset(negative_df['char_id_list'],negative_df['ans_id_list'])\n",
        "total_dataset = Dataset(total_df['char_id_list'],total_df['ans_id_list'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DYp9q3e5ssdM"
      },
      "outputs": [],
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(math_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)\n",
        "train_brackets_data_loader = torch.utils.data.DataLoader(bracket_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)\n",
        "train_no_brackets_data_loader = torch.utils.data.DataLoader(no_brackets_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)\n",
        "train_positive_data_loader = torch.utils.data.DataLoader(positive_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)\n",
        "train_negative_data_loader = torch.utils.data.DataLoader(negative_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)\n",
        "train_total_data_loader = torch.utils.data.DataLoader(total_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)\n",
        "validation_data_loader = torch.utils.data.DataLoader(validation_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuAvp-ZssdM"
      },
      "source": [
        "# 模型設計\n",
        "\n",
        "## 執行順序\n",
        "1. 將句子中的所有字轉換成 embedding\n",
        "2. 按照句子順序將 embedding 丟入 LSTM\n",
        "3. LSTM 的輸出再丟給 LSTM，可以接上更多層\n",
        "4. 最後的 LSTM 所有時間點的輸出丟進一層 Fully Connected\n",
        "5. 輸出結果所有維度中的最大者即為下一個字\n",
        "\n",
        "## 損失函數\n",
        "因為是類別預測，所以使用 Cross Entropy\n",
        "\n",
        "## 梯度更新\n",
        "使用 Adam 演算法進行梯度更新"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fNhof8yVssdM"
      },
      "outputs": [],
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharRNN, self).__init__()\n",
        "\n",
        "        # Embedding層\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
        "                                            embedding_dim=embed_dim,\n",
        "                                            padding_idx=char_to_id['<pad>'])\n",
        "\n",
        "        # RNN層\n",
        "        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,\n",
        "                                        hidden_size=hidden_dim,\n",
        "                                        batch_first=True)\n",
        "\n",
        "        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,\n",
        "                                  hidden_size=hidden_dim,\n",
        "                                  batch_first=True)\n",
        "\n",
        "        # output層\n",
        "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,\n",
        "                                  out_features=hidden_dim),\n",
        "                                  torch.nn.ReLU(),\n",
        "                                  torch.nn.Linear(in_features=hidden_dim,\n",
        "                                  out_features=vocab_size))\n",
        "\n",
        "    def forward(self, batch_x, batch_x_lens):\n",
        "        return self.encoder(batch_x, batch_x_lens)\n",
        "\n",
        "    def encoder(self, batch_x, batch_x_lens):\n",
        "        batch_x = self.embedding(batch_x)\n",
        "        batch_x = torch.nn.utils.rnn.pack_padded_sequence(batch_x,\n",
        "                                                          batch_x_lens,\n",
        "                                                          batch_first=True,\n",
        "                                                          enforce_sorted=False)\n",
        "\n",
        "        batch_x, _ = self.rnn_layer1(batch_x)\n",
        "        batch_x, _ = self.rnn_layer2(batch_x)\n",
        "\n",
        "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x,\n",
        "                                                            batch_first=True)\n",
        "\n",
        "        batch_x = self.linear(batch_x)\n",
        "\n",
        "        return batch_x\n",
        "\n",
        "    def generator(self, start_char, max_len=200):\n",
        "\n",
        "        char_list = [char_to_id[start_char]]\n",
        "\n",
        "        next_char = None\n",
        "\n",
        "        # 生成的長度沒達到max_len就一直生\n",
        "        while len(char_list) < max_len:\n",
        "            x = torch.LongTensor(char_list).unsqueeze(0)\n",
        "            x = self.embedding(x)\n",
        "            _, (ht, _) = self.rnn_layer1(x)\n",
        "            _, (ht, _) = self.rnn_layer2(ht)\n",
        "            y = self.linear(ht)\n",
        "\n",
        "            next_char = np.argmax(y.numpy())\n",
        "            # 如果看到新的token是<eos>就說明生成結束了，就停下\n",
        "            if next_char == char_to_id['<eos>']:\n",
        "                break\n",
        "\n",
        "            char_list.append(next_char)\n",
        "\n",
        "        return [id_to_char[ch_id] for ch_id in char_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J7J_jVDussdN"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(2)\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model = CharRNN(vocab_size,\n",
        "                embed_dim,\n",
        "                hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z2XPQ9W4ssdN"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'], reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXgU5hnpssdN"
      },
      "source": [
        "# 訓練\n",
        "1. 最外層的 `for` 迴圈控制 `epoch`\n",
        "    1. 內層的 `for` 迴圈透過 `data_loader` 取得 batch\n",
        "        1. 丟給 `model` 進行訓練\n",
        "        2. 預測結果 `batch_pred_y` 跟真正的答案 `batch_y` 進行 Cross Entropy 得到誤差 `loss`\n",
        "        3. 使用 `loss.backward` 自動計算梯度\n",
        "        4. 使用 `torch.nn.utils.clip_grad_value_` 將梯度限制在 `-grad_clip` &lt; &lt; `grad_clip` 之間\n",
        "        5. 使用 `optimizer.step()` 進行更新（back propagation）\n",
        "2. 每 `1000` 個 batch 就輸出一次當前的 loss 觀察是否有收斂的趨勢"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jwAaDbGMssdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02de6ba8-4235-4757-eeec-405bc094c5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training brackets epoch 1: 100%|██████████| 12/12 [08:10<00:00, 40.90s/it, loss=1.98]\n",
            "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4 - 1 5 * 1 =\n",
            "pred_y:1 1 \n",
            "batch_y:- 1 1 \n",
            "4 9 * 4 + 1 4 =\n",
            "pred_y:- \n",
            "batch_y:2 1 0 \n",
            "4 - 1 5 =\n",
            "pred_y:1 1 \n",
            "batch_y:- 1 1 \n",
            "4 - ( 1 5 * 1 ) =\n",
            "pred_y:1 \n",
            "batch_y:- 1 1 \n",
            "4 - 1 4 - 4 9 =\n",
            "pred_y:\n",
            "batch_y:- 5 9 \n",
            "1 * ( 4 + 1 5 ) =\n",
            "pred_y:\n",
            "batch_y:1 9 \n",
            "4 - 1 4 * 4 9 =\n",
            "pred_y:\n",
            "batch_y:- 6 8 2 \n",
            "( 4 + 1 5 ) * 0 =\n",
            "pred_y:- 2 2 \n",
            "batch_y:0 \n",
            "0 * 4 + 1 5 =\n",
            "pred_y:2 2 1 \n",
            "batch_y:1 5 \n",
            "4 + 1 4 - 4 8 =\n",
            "pred_y:2 \n",
            "batch_y:- 3 0 \n",
            "4 + ( 1 5 * 1 ) =\n",
            "pred_y:- - \n",
            "batch_y:1 9 \n",
            "4 + ( 1 5 - 0 ) =\n",
            "pred_y:2 \n",
            "batch_y:1 9 \n",
            "( 4 - 1 4 ) + 4 8 =\n",
            "pred_y:1 1 1 \n",
            "batch_y:3 8 \n",
            "4 - 1 5 + 0 =\n",
            "pred_y:\n",
            "batch_y:- 1 1 \n",
            "4 9 * ( 4 + 1 4 ) =\n",
            "pred_y:- \n",
            "batch_y:8 8 2 \n",
            "( 1 5 * 0 ) + 4 =\n",
            "pred_y:\n",
            "batch_y:4 \n",
            "4 + 1 4 * 4 9 =\n",
            "pred_y:- \n",
            "batch_y:6 9 0 \n",
            "( 4 - 1 5 ) * 1 =\n",
            "pred_y:- - \n",
            "batch_y:- 1 1 \n",
            "( 4 + 1 5 ) * 1 =\n",
            "pred_y:- 1 1 \n",
            "batch_y:1 9 \n",
            "4 * 1 4 * 4 8 =\n",
            "pred_y:- 1 1 1 \n",
            "batch_y:2 6 8 8 \n",
            "4 + 1 5 - 0 =\n",
            "pred_y:- 2 2 \n",
            "batch_y:1 9 \n",
            "4 - 1 4 + 4 8 =\n",
            "pred_y:- \n",
            "batch_y:3 8 \n",
            "4 - 1 4 + 4 9 =\n",
            "pred_y:3 \n",
            "batch_y:3 9 \n",
            "( 1 4 * 4 8 ) - 4 =\n",
            "pred_y:\n",
            "batch_y:6 6 8 \n",
            "4 - ( 1 5 * 0 ) =\n",
            "pred_y:1 \n",
            "batch_y:4 \n",
            "4 + 1 4 + 4 9 =\n",
            "pred_y:2 2 \n",
            "batch_y:6 7 \n",
            "4 - 1 4 - 4 8 =\n",
            "pred_y:- 2 1 \n",
            "batch_y:- 5 8 \n",
            "4 * 1 5 =\n",
            "pred_y:- - 1 \n",
            "batch_y:6 0 \n",
            "4 - 1 5 * 0 =\n",
            "pred_y:3 \n",
            "batch_y:4 \n",
            "4 9 * ( 4 - 1 4 ) =\n",
            "pred_y:- \n",
            "batch_y:- 4 9 0 \n",
            "( 1 5 * 0 ) - 4 =\n",
            "pred_y:2 1 \n",
            "batch_y:- 4 \n",
            "1 * 4 + 1 5 =\n",
            "pred_y:- \n",
            "batch_y:1 9 \n",
            "4 + 1 5 * 1 =\n",
            "pred_y:2 \n",
            "batch_y:1 9 \n",
            "4 * 1 5 * 0 =\n",
            "pred_y:1 1 \n",
            "batch_y:0 \n",
            "4 * 1 4 * 4 9 =\n",
            "pred_y:\n",
            "batch_y:2 7 4 4 \n",
            "4 - ( 1 4 * 4 9 ) =\n",
            "pred_y:\n",
            "batch_y:- 6 8 2 \n",
            "0 * ( 4 + 1 5 ) =\n",
            "pred_y:\n",
            "batch_y:0 \n",
            "( 4 - 1 5 ) * 0 =\n",
            "pred_y:1 \n",
            "batch_y:0 \n",
            "( 4 - 1 4 ) + 4 9 =\n",
            "pred_y:\n",
            "batch_y:3 9 \n",
            "4 + 1 4 + 4 8 =\n",
            "pred_y:- - \n",
            "batch_y:6 6 \n",
            "4 + 1 5 * 0 =\n",
            "pred_y:- - - \n",
            "batch_y:4 \n",
            "4 + ( 1 4 * 4 9 ) =\n",
            "pred_y:\n",
            "batch_y:6 9 0 \n",
            "4 + ( 1 4 - 4 8 ) =\n",
            "pred_y:2 2 \n",
            "batch_y:- 3 0 \n",
            "( 4 + 1 4 ) * 4 9 =\n",
            "pred_y:- - \n",
            "batch_y:8 8 2 \n",
            "0 * 4 - 1 5 =\n",
            "pred_y:2 2 \n",
            "batch_y:- 1 5 \n",
            "4 - 1 5 - 0 =\n",
            "pred_y:- \n",
            "batch_y:- 1 1 \n",
            "1 * ( 4 - 1 5 ) =\n",
            "pred_y:- \n",
            "batch_y:- 1 1 \n",
            "4 + ( 1 5 * 0 ) =\n",
            "pred_y:\n",
            "batch_y:4 \n",
            "4 + ( 1 4 - 4 9 ) =\n",
            "pred_y:\n",
            "batch_y:- 3 1 \n",
            "0 * ( 4 - 1 5 ) =\n",
            "pred_y:- 1 \n",
            "batch_y:0 \n",
            "4 - ( 1 4 + 4 8 ) =\n",
            "pred_y:- \n",
            "batch_y:- 5 8 \n",
            "4 - ( 1 5 + 0 ) =\n",
            "pred_y:- 1 1 \n",
            "batch_y:- 1 1 \n",
            "( 1 4 * 4 9 ) + 4 =\n",
            "pred_y:2 1 \n",
            "batch_y:6 9 0 \n",
            "( 4 + 1 5 ) - 0 =\n",
            "pred_y:- 1 1 \n",
            "batch_y:1 9 \n",
            "( 4 + 1 4 ) - 4 9 =\n",
            "pred_y:- 2 \n",
            "batch_y:- 3 1 \n",
            "4 + 1 5 =\n",
            "pred_y:- - \n",
            "batch_y:1 9 \n",
            "( 4 + 1 4 ) - 4 8 =\n",
            "pred_y:- 2 2 \n",
            "batch_y:- 3 0 \n",
            "( 1 4 * 4 9 ) - 4 =\n",
            "pred_y:- \n",
            "batch_y:6 8 2 \n",
            "4 + 1 5 + 0 =\n",
            "pred_y:1 1 1 \n",
            "batch_y:1 9 \n",
            "( 4 - 1 5 ) + 0 =\n",
            "pred_y:- - 1 1 \n",
            "batch_y:- 1 1 \n",
            "4 - ( 1 4 + 4 9 ) =\n",
            "pred_y:1 1 \n",
            "batch_y:- 5 9 \n",
            "4 9 * 4 - 1 4 =\n",
            "pred_y:- \n",
            "batch_y:1 8 2 \n",
            "4 + 1 4 - 4 9 =\n",
            "pred_y:2 2 2 \n",
            "batch_y:- 3 1 \n",
            "( 4 - 1 4 ) * 4 9 =\n",
            "pred_y:- - 1 1 \n",
            "batch_y:- 4 9 0 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.7141104936599731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training brackets epoch 2: 100%|██████████| 12/12 [08:14<00:00, 41.21s/it, loss=0.734]\n",
            "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4 9 * 4 + 1 4 =\n",
            "pred_y:1 6 8 \n",
            "batch_y:2 1 0 \n",
            "4 * 1 4 * 4 9 =\n",
            "pred_y:- 2 2 \n",
            "batch_y:2 7 4 4 \n",
            "4 - ( 1 4 * 4 9 ) =\n",
            "pred_y:2 6 3 3 \n",
            "batch_y:- 6 8 2 \n",
            "4 * 1 5 =\n",
            "pred_y:- 4 3 \n",
            "batch_y:6 0 \n",
            "4 - 1 5 * 1 =\n",
            "pred_y:0 \n",
            "batch_y:- 1 1 \n",
            "( 4 - 1 5 ) * 0 =\n",
            "pred_y:1 5 1 9 \n",
            "batch_y:0 \n",
            "( 1 5 * 0 ) + 4 =\n",
            "pred_y:2 8 \n",
            "batch_y:4 \n",
            "4 - 1 5 =\n",
            "pred_y:3 3 3 \n",
            "batch_y:- 1 1 \n",
            "0 * 4 - 1 5 =\n",
            "pred_y:- 4 2 \n",
            "batch_y:- 1 5 \n",
            "( 4 + 1 5 ) - 0 =\n",
            "pred_y:1 2 1 1 \n",
            "batch_y:1 9 \n",
            "( 4 + 1 5 ) * 0 =\n",
            "pred_y:3 1 3 6 \n",
            "batch_y:0 \n",
            "4 + 1 4 - 4 9 =\n",
            "pred_y:- 1 3 2 \n",
            "batch_y:- 3 1 \n",
            "4 - 1 4 - 4 9 =\n",
            "pred_y:7 2 \n",
            "batch_y:- 5 9 \n",
            "4 + 1 5 * 1 =\n",
            "pred_y:5 2 \n",
            "batch_y:1 9 \n",
            "( 1 4 * 4 9 ) - 4 =\n",
            "pred_y:3 8 2 7 \n",
            "batch_y:6 8 2 \n",
            "4 * 1 4 * 4 8 =\n",
            "pred_y:4 4 5 3 \n",
            "batch_y:2 6 8 8 \n",
            "4 + 1 4 - 4 8 =\n",
            "pred_y:5 2 2 \n",
            "batch_y:- 3 0 \n",
            "4 + 1 5 + 0 =\n",
            "pred_y:5 6 \n",
            "batch_y:1 9 \n",
            "4 + ( 1 4 * 4 9 ) =\n",
            "pred_y:- 1 3 1 \n",
            "batch_y:6 9 0 \n",
            "4 - 1 5 + 0 =\n",
            "pred_y:3 3 \n",
            "batch_y:- 1 1 \n",
            "4 9 * ( 4 + 1 4 ) =\n",
            "pred_y:6 3 \n",
            "batch_y:8 8 2 \n",
            "4 - ( 1 4 + 4 9 ) =\n",
            "pred_y:5 4 \n",
            "batch_y:- 5 9 \n",
            "4 - ( 1 5 * 1 ) =\n",
            "pred_y:4 4 \n",
            "batch_y:- 1 1 \n",
            "4 + 1 5 =\n",
            "pred_y:6 0 \n",
            "batch_y:1 9 \n",
            "1 * 4 + 1 5 =\n",
            "pred_y:1 6 \n",
            "batch_y:1 9 \n",
            "( 4 - 1 4 ) * 4 9 =\n",
            "pred_y:- 4 2 \n",
            "batch_y:- 4 9 0 \n",
            "4 + 1 5 * 0 =\n",
            "pred_y:1 5 1 1 \n",
            "batch_y:4 \n",
            "( 4 + 1 4 ) - 4 8 =\n",
            "pred_y:- 8 1 \n",
            "batch_y:- 3 0 \n",
            "( 4 + 1 5 ) * 1 =\n",
            "pred_y:- 3 \n",
            "batch_y:1 9 \n",
            "( 4 - 1 4 ) + 4 9 =\n",
            "pred_y:1 2 0 \n",
            "batch_y:3 9 \n",
            "4 + ( 1 5 - 0 ) =\n",
            "pred_y:- 1 1 7 \n",
            "batch_y:1 9 \n",
            "( 4 - 1 5 ) + 0 =\n",
            "pred_y:1 2 4 4 \n",
            "batch_y:- 1 1 \n",
            "4 + ( 1 4 - 4 8 ) =\n",
            "pred_y:1 8 0 \n",
            "batch_y:- 3 0 \n",
            "4 - 1 5 - 0 =\n",
            "pred_y:- 3 2 \n",
            "batch_y:- 1 1 \n",
            "4 - ( 1 5 + 0 ) =\n",
            "pred_y:8 0 \n",
            "batch_y:- 1 1 \n",
            "4 + 1 5 - 0 =\n",
            "pred_y:- 3 3 \n",
            "batch_y:1 9 \n",
            "( 4 + 1 4 ) * 4 9 =\n",
            "pred_y:4 0 \n",
            "batch_y:8 8 2 \n",
            "4 - 1 5 * 0 =\n",
            "pred_y:0 \n",
            "batch_y:4 \n",
            "0 * ( 4 + 1 5 ) =\n",
            "pred_y:2 0 \n",
            "batch_y:0 \n",
            "4 + 1 4 * 4 9 =\n",
            "pred_y:2 6 6 \n",
            "batch_y:6 9 0 \n",
            "( 4 - 1 5 ) * 1 =\n",
            "pred_y:- 5 0 \n",
            "batch_y:- 1 1 \n",
            "4 9 * ( 4 - 1 4 ) =\n",
            "pred_y:- 1 0 8 \n",
            "batch_y:- 4 9 0 \n",
            "4 - 1 4 + 4 9 =\n",
            "pred_y:6 6 \n",
            "batch_y:3 9 \n",
            "4 + 1 4 + 4 9 =\n",
            "pred_y:- 1 \n",
            "batch_y:6 7 \n",
            "4 + ( 1 5 * 0 ) =\n",
            "pred_y:1 6 \n",
            "batch_y:4 \n",
            "4 + ( 1 5 * 1 ) =\n",
            "pred_y:1 1 5 \n",
            "batch_y:1 9 \n",
            "4 9 * 4 - 1 4 =\n",
            "pred_y:1 2 \n",
            "batch_y:1 8 2 \n",
            "( 1 4 * 4 9 ) + 4 =\n",
            "pred_y:3 8 4 \n",
            "batch_y:6 9 0 \n",
            "4 + ( 1 4 - 4 9 ) =\n",
            "pred_y:1 1 \n",
            "batch_y:- 3 1 \n",
            "4 - 1 4 - 4 8 =\n",
            "pred_y:- 3 5 \n",
            "batch_y:- 5 8 \n",
            "0 * ( 4 - 1 5 ) =\n",
            "pred_y:2 2 0 \n",
            "batch_y:0 \n",
            "4 - 1 4 * 4 9 =\n",
            "pred_y:- 7 0 \n",
            "batch_y:- 6 8 2 \n",
            "4 - ( 1 4 + 4 8 ) =\n",
            "pred_y:4 4 \n",
            "batch_y:- 5 8 \n",
            "( 4 - 1 4 ) + 4 8 =\n",
            "pred_y:1 8 4 \n",
            "batch_y:3 8 \n",
            "1 * ( 4 - 1 5 ) =\n",
            "pred_y:8 0 \n",
            "batch_y:- 1 1 \n",
            "0 * 4 + 1 5 =\n",
            "pred_y:- 4 0 \n",
            "batch_y:1 5 \n",
            "( 1 5 * 0 ) - 4 =\n",
            "pred_y:3 6 4 4 \n",
            "batch_y:- 4 \n",
            "4 + 1 4 + 4 8 =\n",
            "pred_y:1 9 \n",
            "batch_y:6 6 \n",
            "1 * ( 4 + 1 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 ) =\n",
            "pred_y:9 9 4 \n",
            "batch_y:1 9 \n",
            "4 * 1 5 * 0 =\n",
            "pred_y:1 9 0 \n",
            "batch_y:0 \n",
            "( 4 + 1 4 ) - 4 9 =\n",
            "pred_y:- 3 0 \n",
            "batch_y:- 3 1 \n",
            "4 - ( 1 5 * 0 ) =\n",
            "pred_y:7 3 \n",
            "batch_y:4 \n",
            "( 1 4 * 4 8 ) - 4 =\n",
            "pred_y:3 8 \n",
            "batch_y:6 6 8 \n",
            "4 - 1 4 + 4 8 =\n",
            "pred_y:- 2 2 \n",
            "batch_y:3 8 \n",
            "Validation Loss: 0.5358591079711914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training brackets epoch 3: 100%|██████████| 12/12 [08:21<00:00, 41.79s/it, loss=0.0614]\n",
            "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4 + ( 1 5 * 0 ) =\n",
            "pred_y:0 \n",
            "batch_y:4 \n",
            "( 4 - 1 5 ) * 1 =\n",
            "pred_y:3 8 4 \n",
            "batch_y:- 1 1 \n",
            "4 - 1 5 * 1 =\n",
            "pred_y:3 1 1 5 \n",
            "batch_y:- 1 1 \n",
            "0 * 4 + 1 5 =\n",
            "pred_y:- 1 5 0 \n",
            "batch_y:1 5 \n",
            "4 - ( 1 4 + 4 8 ) =\n",
            "pred_y:9 \n",
            "batch_y:- 5 8 \n",
            "1 * 4 + 1 5 =\n",
            "pred_y:5 0 \n",
            "batch_y:1 9 \n",
            "4 + 1 5 * 0 =\n",
            "pred_y:- 1 4 5 5 \n",
            "batch_y:4 \n",
            "4 9 * ( 4 + 1 4 ) =\n",
            "pred_y:2 0 \n",
            "batch_y:8 8 2 \n",
            "4 + ( 1 5 * 1 ) =\n",
            "pred_y:9 2 \n",
            "batch_y:1 9 \n",
            "0 * 4 - 1 5 =\n",
            "pred_y:- 7 8 \n",
            "batch_y:- 1 5 \n",
            "4 9 * ( 4 - 1 4 ) =\n",
            "pred_y:6 5 8 \n",
            "batch_y:- 4 9 0 \n",
            "( 4 - 1 4 ) * 4 9 =\n",
            "pred_y:- 3 0 \n",
            "batch_y:- 4 9 0 \n",
            "( 4 - 1 4 ) + 4 8 =\n",
            "pred_y:4 6 \n",
            "batch_y:3 8 \n",
            "4 + ( 1 4 - 4 9 ) =\n",
            "pred_y:1 9 2 \n",
            "batch_y:- 3 1 \n",
            "4 + ( 1 5 - 0 ) =\n",
            "pred_y:- 1 4 4 \n",
            "batch_y:1 9 \n",
            "( 4 + 1 4 ) - 4 9 =\n",
            "pred_y:- 5 \n",
            "batch_y:- 3 1 \n",
            "4 + 1 4 + 4 8 =\n",
            "pred_y:- 3 5 \n",
            "batch_y:6 6 \n",
            "4 + ( 1 4 - 4 8 ) =\n",
            "pred_y:- 1 4 \n",
            "batch_y:- 3 0 \n",
            "( 1 4 * 4 9 ) - 4 =\n",
            "pred_y:- 3 \n",
            "batch_y:6 8 2 \n",
            "4 * 1 5 =\n",
            "pred_y:4 0 5 0 \n",
            "batch_y:6 0 \n",
            "4 9 * 4 + 1 4 =\n",
            "pred_y:- 8 2 \n",
            "batch_y:2 1 0 \n",
            "4 + 1 5 + 0 =\n",
            "pred_y:- 1 4 4 \n",
            "batch_y:1 9 \n",
            "( 1 5 * 0 ) - 4 =\n",
            "pred_y:1 7 1 0 \n",
            "batch_y:- 4 \n",
            "4 + 1 5 - 0 =\n",
            "pred_y:4 0 \n",
            "batch_y:1 9 \n",
            "( 4 + 1 5 ) * 0 =\n",
            "pred_y:4 4 \n",
            "batch_y:0 \n",
            "( 1 4 * 4 9 ) + 4 =\n",
            "pred_y:0 \n",
            "batch_y:6 9 0 \n",
            "4 + 1 5 =\n",
            "pred_y:0 \n",
            "batch_y:1 9 \n",
            "4 - ( 1 5 * 0 ) =\n",
            "pred_y:- 9 \n",
            "batch_y:4 \n",
            "4 - ( 1 5 * 1 ) =\n",
            "pred_y:- 1 0 \n",
            "batch_y:- 1 1 \n",
            "( 4 - 1 5 ) * 0 =\n",
            "pred_y:- 9 \n",
            "batch_y:0 \n",
            "4 - 1 5 - 0 =\n",
            "pred_y:- 4 7 \n",
            "batch_y:- 1 1 \n",
            "4 + 1 4 + 4 9 =\n",
            "pred_y:7 0 \n",
            "batch_y:6 7 \n",
            "4 - 1 5 + 0 =\n",
            "pred_y:3 7 6 0 \n",
            "batch_y:- 1 1 \n",
            "4 - 1 4 - 4 9 =\n",
            "pred_y:9 1 \n",
            "batch_y:- 5 9 \n",
            "4 - ( 1 4 * 4 9 ) =\n",
            "pred_y:1 8 2 5 \n",
            "batch_y:- 6 8 2 \n",
            "4 - ( 1 5 + 0 ) =\n",
            "pred_y:3 0 3 8 \n",
            "batch_y:- 1 1 \n",
            "4 + ( 1 4 * 4 9 ) =\n",
            "pred_y:3 2 6 4 \n",
            "batch_y:6 9 0 \n",
            "4 - ( 1 4 + 4 9 ) =\n",
            "pred_y:- 1 4 \n",
            "batch_y:- 5 9 \n",
            "0 * ( 4 + 1 5 ) =\n",
            "pred_y:0 \n",
            "batch_y:0 \n",
            "( 4 - 1 5 ) + 0 =\n",
            "pred_y:- 1 3 \n",
            "batch_y:- 1 1 \n",
            "0 * ( 4 - 1 5 ) =\n",
            "pred_y:- 4 1 \n",
            "batch_y:0 \n",
            "("
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 4 + 1 4 ) - 4 8 =\n",
            "pred_y:2 6 1 0 \n",
            "batch_y:- 3 0 \n",
            "4 * 1 5 * 0 =\n",
            "pred_y:- 5 1 \n",
            "batch_y:0 \n",
            "( 1 5 * 0 ) + 4 =\n",
            "pred_y:- 1 6 7 3 \n",
            "batch_y:4 \n",
            "4 + 1 4 - 4 8 =\n",
            "pred_y:1 9 2 \n",
            "batch_y:- 3 0 \n",
            "4 - 1 4 * 4 9 =\n",
            "pred_y:0 \n",
            "batch_y:- 6 8 2 \n",
            "( 4 + 1 4 ) * 4 9 =\n",
            "pred_y:3 2 0 \n",
            "batch_y:8 8 2 \n",
            "4 - 1 4 + 4 9 =\n",
            "pred_y:- 5 \n",
            "batch_y:3 9 \n",
            "4 * 1 4 * 4 8 =\n",
            "pred_y:- 1 6 5 3 \n",
            "batch_y:2 6 8 8 \n",
            "4 9 * 4 - 1 4 =\n",
            "pred_y:3 1 \n",
            "batch_y:1 8 2 \n",
            "( 4 + 1 5 ) - 0 =\n",
            "pred_y:2 1 1 6 \n",
            "batch_y:1 9 \n",
            "4 + 1 4 * 4 9 =\n",
            "pred_y:7 9 \n",
            "batch_y:6 9 0 \n",
            "( 4 - 1 4 ) + 4 9 =\n",
            "pred_y:- 1 9 2 \n",
            "batch_y:3 9 \n",
            "4 - 1 5 =\n",
            "pred_y:1 8 \n",
            "batch_y:- 1 1 \n",
            "4 + 1 5 * 1 =\n",
            "pred_y:2 0 2 4 \n",
            "batch_y:1 9 \n",
            "1 * ( 4 + 1 5 ) =\n",
            "pred_y:3 8 0 \n",
            "batch_y:1 9 \n",
            "4 - 1 4 - 4 8 =\n",
            "pred_y:- 3 \n",
            "batch_y:- 5 8 \n",
            "4 - 1 5 * 0 =\n",
            "pred_y:0 \n",
            "batch_y:4 \n",
            "4 + 1 4 - 4 9 =\n",
            "pred_y:- 3 2 \n",
            "batch_y:- 3 1 \n",
            "( 4 + 1 5 ) * 1 =\n",
            "pred_y:1 7 \n",
            "batch_y:1 9 \n",
            "( 1 4 * 4 8 ) - 4 =\n",
            "pred_y:3 0 \n",
            "batch_y:6 6 8 \n",
            "4 * 1 4 * 4 9 =\n",
            "pred_y:0 \n",
            "batch_y:2 7 4 4 \n",
            "1 * ( 4 - 1 5 ) =\n",
            "pred_y:0 \n",
            "batch_y:- 1 1 \n",
            "4 - 1 4 + 4 8 =\n",
            "pred_y:1 1 \n",
            "batch_y:3 8 \n",
            "Validation Loss: 0.03807859867811203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "i = 0\n",
        "total_loss = 0.0\n",
        "num_batches = 0\n",
        "for epoch in range(1, epochs+1):\n",
        "    process_bar = tqdm(train_brackets_data_loader, desc=f\"Training brackets epoch {epoch}\")\n",
        "    for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "        # 標準DL訓練幾板斧\n",
        "        optimizer.zero_grad()\n",
        "        batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "\n",
        "        # 找batch_x的等號位置\n",
        "        equ_pos = []\n",
        "        for tensor in batch_x:\n",
        "          for idx, value in enumerate(tensor):\n",
        "            if value.item() == char_to_id['=']:\n",
        "                equ_pos.append(idx)\n",
        "        # 把從18個字元中，找出機率最大的就是預測的字元\n",
        "        tensor_y = torch.zeros_like(batch_y)\n",
        "        for i in range(0,batch_y.size(0)):\n",
        "          idx = equ_pos[i]+1\n",
        "          for j in range(0,batch_y.size(1)):\n",
        "            if j+idx >= batch_pred_y.size(1):\n",
        "              break\n",
        "            tensor_y[i][j]=torch.argmax(batch_pred_y[i][idx+j],0)\n",
        "        # 看tensor_pred_y和batch_y\n",
        "        # for X,tensor,batch in zip(batch_x,tensor_y,batch_y):\n",
        "        #   for idx in X:\n",
        "        #     if id_to_char[idx.item()] == '=':\n",
        "        #       print(id_to_char[idx.item()])\n",
        "        #       break\n",
        "        #     print(id_to_char[idx.item()],end=' ')\n",
        "        #   print('pred_y:',end='')\n",
        "        #   for val in tensor:\n",
        "        #     if id_to_char[val.item()] == '<eos>':\n",
        "        #       break\n",
        "        #     print(id_to_char[val.item()],end=\" \")\n",
        "        #   print()\n",
        "        #   print('batch_y:',end='')\n",
        "        #   for val in batch:\n",
        "        #     if id_to_char[val.item()] == '<eos>':\n",
        "        #       break\n",
        "        #     print(id_to_char[val.item()],end=\" \")\n",
        "        #   print()\n",
        "        tensor_pred_y = torch.empty(batch_y.size(0),batch_y.size(1),18)\n",
        "        # 把等號後面部分取出來\n",
        "        for i in range(0,batch_y.size(0)):\n",
        "          equ = equ_pos[i]+1\n",
        "          for j in range(0,batch_y.size(1)):\n",
        "            if j+equ >= batch_pred_y.size(1):\n",
        "              break\n",
        "            for k in range(0,18):\n",
        "              # print(batch_pred_y[i][equ+j][k])\n",
        "              tensor_pred_y[i][j][k] = batch_pred_y[i][equ+j][k]\n",
        "        tensor_pred_y = tensor_pred_y.view(-1,vocab_size)\n",
        "        batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "        loss = criterion(tensor_pred_y,batch_y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        i+=1\n",
        "        if i%10==0:\n",
        "          process_bar.set_postfix(loss=loss.item())\n",
        "#  嘗試在一個epoch中用不同資料訓練\n",
        "#     process_bar = tqdm(train_no_brackets_data_loader, desc=f\"Training no_brackets epoch {epoch}\")\n",
        "#     for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "#         # 標準DL訓練幾板斧\n",
        "#         optimizer.zero_grad()\n",
        "#         batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "\n",
        "#         # 找batch_x的等號位置\n",
        "#         equ_pos = []\n",
        "#         for tensor in batch_x:\n",
        "#           for idx, value in enumerate(tensor):\n",
        "#             if value.item() == char_to_id['=']:\n",
        "#                 equ_pos.append(idx)\n",
        "#         # 把從18個字元中，找出機率最大的就是預測的字元\n",
        "#         tensor_y = torch.zeros_like(batch_y)\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           idx = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+idx >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             tensor_y[i][j]=torch.argmax(batch_pred_y[i][idx+j],0)\n",
        "#         tensor_pred_y = torch.empty(batch_y.size(0),batch_y.size(1),18)\n",
        "#         # 把等號後面部分取出來\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           equ = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+equ >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             for k in range(0,18):\n",
        "#               # print(batch_pred_y[i][equ+j][k])\n",
        "#               tensor_pred_y[i][j][k] = batch_pred_y[i][equ+j][k]\n",
        "#         tensor_pred_y = tensor_pred_y.view(-1,vocab_size)\n",
        "#         batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "#         loss = criterion(tensor_pred_y,batch_y)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         num_batches += 1\n",
        "#         i+=1\n",
        "#         if i%10==0:\n",
        "#           process_bar.set_postfix(loss=loss.item())\n",
        "#     process_bar = tqdm(train_positive_data_loader, desc=f\"Training positive epoch {epoch}\")\n",
        "#     for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "#         # 標準DL訓練幾板斧\n",
        "#         optimizer.zero_grad()\n",
        "#         batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "\n",
        "#         # 找batch_x的等號位置\n",
        "#         equ_pos = []\n",
        "#         for tensor in batch_x:\n",
        "#           for idx, value in enumerate(tensor):\n",
        "#             if value.item() == char_to_id['=']:\n",
        "#                 equ_pos.append(idx)\n",
        "#         # 把從18個字元中，找出機率最大的就是預測的字元\n",
        "#         tensor_y = torch.zeros_like(batch_y)\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           idx = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+idx >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             tensor_y[i][j]=torch.argmax(batch_pred_y[i][idx+j],0)\n",
        "#         tensor_pred_y = torch.empty(batch_y.size(0),batch_y.size(1),18)\n",
        "#         # 把等號後面部分取出來\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           equ = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+equ >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             for k in range(0,18):\n",
        "#               # print(batch_pred_y[i][equ+j][k])\n",
        "#               tensor_pred_y[i][j][k] = batch_pred_y[i][equ+j][k]\n",
        "#         tensor_pred_y = tensor_pred_y.view(-1,vocab_size)\n",
        "#         batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "#         loss = criterion(tensor_pred_y,batch_y)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         num_batches += 1\n",
        "#         i+=1\n",
        "#         if i%10==0:\n",
        "#           process_bar.set_postfix(loss=loss.item())\n",
        "#     process_bar = tqdm(train_negative_data_loader, desc=f\"Training negative epoch {epoch}\")\n",
        "#     for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "#         # 標準DL訓練幾板斧\n",
        "#         optimizer.zero_grad()\n",
        "#         batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "\n",
        "#         # 找batch_x的等號位置\n",
        "#         equ_pos = []\n",
        "#         for tensor in batch_x:\n",
        "#           for idx, value in enumerate(tensor):\n",
        "#             if value.item() == char_to_id['=']:\n",
        "#                 equ_pos.append(idx)\n",
        "#         # 把從18個字元中，找出機率最大的就是預測的字元\n",
        "#         tensor_y = torch.zeros_like(batch_y)\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           idx = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+idx >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             tensor_y[i][j]=torch.argmax(batch_pred_y[i][idx+j],0)\n",
        "#         tensor_pred_y = torch.empty(batch_y.size(0),batch_y.size(1),18)\n",
        "#         # 把等號後面部分取出來\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           equ = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+equ >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             for k in range(0,18):\n",
        "#               # print(batch_pred_y[i][equ+j][k])\n",
        "#               tensor_pred_y[i][j][k] = batch_pred_y[i][equ+j][k]\n",
        "#         tensor_pred_y = tensor_pred_y.view(-1,vocab_size)\n",
        "#         batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "#         loss = criterion(tensor_pred_y,batch_y)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         num_batches += 1\n",
        "#         i+=1\n",
        "#         if i%10==0:\n",
        "#           process_bar.set_postfix(loss=loss.item())\n",
        "#     process_bar = tqdm(train_total_data_loader, desc=f\"Training total epoch {epoch}\")\n",
        "#     for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "#         # 標準DL訓練幾板斧\n",
        "#         optimizer.zero_grad()\n",
        "#         batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "\n",
        "#         # 找batch_x的等號位置\n",
        "#         equ_pos = []\n",
        "#         for tensor in batch_x:\n",
        "#           for idx, value in enumerate(tensor):\n",
        "#             if value.item() == char_to_id['=']:\n",
        "#                 equ_pos.append(idx)\n",
        "#         # 把從18個字元中，找出機率最大的就是預測的字元\n",
        "#         tensor_y = torch.zeros_like(batch_y)\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           idx = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+idx >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             tensor_y[i][j]=torch.argmax(batch_pred_y[i][idx+j],0)\n",
        "#         tensor_pred_y = torch.empty(batch_y.size(0),batch_y.size(1),18)\n",
        "#         # 把等號後面部分取出來\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           equ = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+equ >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             for k in range(0,18):\n",
        "#               # print(batch_pred_y[i][equ+j][k])\n",
        "#               tensor_pred_y[i][j][k] = batch_pred_y[i][equ+j][k]\n",
        "#         tensor_pred_y = tensor_pred_y.view(-1,vocab_size)\n",
        "#         batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "#         loss = criterion(tensor_pred_y,batch_y)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         num_batches += 1\n",
        "#         i+=1\n",
        "#         if i%10==0:\n",
        "#           process_bar.set_postfix(loss=loss.item())\n",
        "#     process_bar = tqdm(train_data_loader, desc=f\"Training math epoch {epoch}\")\n",
        "#     for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "#         # 標準DL訓練幾板斧\n",
        "#         optimizer.zero_grad()\n",
        "#         batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "\n",
        "#         # 找batch_x的等號位置\n",
        "#         equ_pos = []\n",
        "#         for tensor in batch_x:\n",
        "#           for idx, value in enumerate(tensor):\n",
        "#             if value.item() == char_to_id['=']:\n",
        "#                 equ_pos.append(idx)\n",
        "#         # 把從18個字元中，找出機率最大的就是預測的字元\n",
        "#         tensor_y = torch.zeros_like(batch_y)\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           idx = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+idx >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             tensor_y[i][j]=torch.argmax(batch_pred_y[i][idx+j],0)\n",
        "#         tensor_pred_y = torch.empty(batch_y.size(0),batch_y.size(1),18)\n",
        "#         # 把等號後面部分取出來\n",
        "#         for i in range(0,batch_y.size(0)):\n",
        "#           equ = equ_pos[i]+1\n",
        "#           for j in range(0,batch_y.size(1)):\n",
        "#             if j+equ >= batch_pred_y.size(1):\n",
        "#               break\n",
        "#             for k in range(0,18):\n",
        "#               # print(batch_pred_y[i][equ+j][k])\n",
        "#               tensor_pred_y[i][j][k] = batch_pred_y[i][equ+j][k]\n",
        "#         tensor_pred_y = tensor_pred_y.view(-1,vocab_size)\n",
        "#         batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "#         loss = criterion(tensor_pred_y,batch_y)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         num_batches += 1\n",
        "#         i+=1\n",
        "#         if i%10==0:\n",
        "#           process_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "#     average_loss = total_loss / num_batches\n",
        "#     print(f\"Training Loss: {average_loss}\")\n",
        "    # 麻煩各位同學加上 validation 的部分\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    validation_process_bar = tqdm(validation_data_loader,desc=\"Validation\")\n",
        "    for batch_x, batch_y, batch_x_lens, batch_y_lens in validation_process_bar:\n",
        "        # optimizer.zero_grad()\n",
        "        batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "        print()\n",
        "        # 找等号位置\n",
        "        equ_pos = []\n",
        "        for tensor in batch_x:\n",
        "            for idx, value in enumerate(tensor):\n",
        "                if value.item() == char_to_id['=']:\n",
        "                    equ_pos.append(idx)\n",
        "\n",
        "        tensor_pred_y = torch.empty(batch_y.size(0), batch_y.size(1), 18)\n",
        "        # 把等號後面部分取出來\n",
        "        for i in range(0, batch_y.size(0)):\n",
        "            equ = equ_pos[i]+1\n",
        "            for j in range(0, batch_y.size(1)):\n",
        "                if j+equ >= batch_pred_y.size(1):\n",
        "                 break\n",
        "                for k in range(0, 18):\n",
        "                    tensor_pred_y[i][j][k] = batch_pred_y[i][equ + j][k]\n",
        "        # 看tensor_pred_y和batch_y\n",
        "        for X,tensor,batch in zip(batch_x,tensor_y,batch_y):\n",
        "          for idx in X:\n",
        "            if id_to_char[idx.item()] == '=':\n",
        "              print(id_to_char[idx.item()])\n",
        "              break\n",
        "            print(id_to_char[idx.item()],end=' ')\n",
        "          print('pred_y:',end='')\n",
        "          for val in tensor:\n",
        "            if id_to_char[val.item()] == '<eos>':\n",
        "              break\n",
        "            print(id_to_char[val.item()],end=\" \")\n",
        "          print()\n",
        "          print('batch_y:',end='')\n",
        "          for val in batch:\n",
        "            if id_to_char[val.item()] == '<eos>':\n",
        "              break\n",
        "            print(id_to_char[val.item()],end=\" \")\n",
        "          print()\n",
        "        tensor_pred_y = tensor_pred_y.view(-1, vocab_size)\n",
        "        batch_y = batch_y.view(-1).to(device)\n",
        "\n",
        "        loss = criterion(tensor_pred_y, batch_y)\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    average_loss = total_loss / num_batches\n",
        "    print(f\"Validation Loss: {average_loss}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}