{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-RNN-文本生成\n",
    "## 教學目標\n",
    "使用 RNN 弄出一個基本的生成文字模型，幫助初學者上手 RNN\n",
    "\n",
    "## 適用對象\n",
    "適用於已經學過 PyTorch 基本語法的人\n",
    "\n",
    "## 執行方法\n",
    "在 Jupyter notebook 中，選取想要執行的區塊後，使用以下其中一種方法執行\n",
    "\n",
    "- 上方工具列中，按下 Cell < Run Cells 執行\n",
    "- 使用快捷鍵 Shift + Enter 執行\n",
    "\n",
    "## 大綱\n",
    "- [載入資料](#載入資料)\n",
    "- [前處理](#前處理)\n",
    "- [建立字典](#建立字典)\n",
    "- [超參數](#超參數)\n",
    "- [資料分批](#資料分批)\n",
    "- [模型設計](#模型設計)\n",
    "- [訓練](#訓練)\n",
    "- [生成](#生成)\n",
    "\n",
    "## 檔案來源\n",
    "- [Kaggle HC 新聞資料集](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)\n",
    "- 下載後請放到路徑 `專案資料夾/data/old-newspaper.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import opencc\n",
    "\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入資料\n",
    "- 請務必先[下載](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)資料後將資料放置到 `data` 資料夾之下\n",
    "- `tsv` 檔案類似 `csv`，只是用 `\\t` 做分隔符號\n",
    "- 資料內容包含\n",
    "\n",
    "|欄位|意義|資料型態|\n",
    "|-|-|-|\n",
    "|`Language`|語系|文字（類別）|\n",
    "|`Source`|新聞來源|文字|\n",
    "|`Date`|時間|文字|\n",
    "|`Text`|文字內容|文字|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Source</th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>republikein.com.na</td>\n",
       "      <td>2011/09/14</td>\n",
       "      <td>Die veranderinge aan die Britsgeboude Avensis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>republikein.com.na</td>\n",
       "      <td>2011/01/20</td>\n",
       "      <td>Duitsland se mans- en vrouespanne is die afgel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>sake24.com</td>\n",
       "      <td>2009/11/28</td>\n",
       "      <td>Mnr. Estienne de Klerk, uitvoerende direkteur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>sake24.com</td>\n",
       "      <td>2009/11/12</td>\n",
       "      <td>Mustek is se finansiële-resultate-advertensie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>sake24.com</td>\n",
       "      <td>2011/02/04</td>\n",
       "      <td>nadat LMS se raad van trustees in Junie verled...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Language              Source        Date  \\\n",
       "0  Afrikaans  republikein.com.na  2011/09/14   \n",
       "1  Afrikaans  republikein.com.na  2011/01/20   \n",
       "2  Afrikaans          sake24.com  2009/11/28   \n",
       "3  Afrikaans          sake24.com  2009/11/12   \n",
       "4  Afrikaans          sake24.com  2011/02/04   \n",
       "\n",
       "                                                Text  \n",
       "0  Die veranderinge aan die Britsgeboude Avensis ...  \n",
       "1  Duitsland se mans- en vrouespanne is die afgel...  \n",
       "2  Mnr. Estienne de Klerk, uitvoerende direkteur ...  \n",
       "3  Mustek is se finansiële-resultate-advertensie ...  \n",
       "4  nadat LMS se raad van trustees in Junie verled...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_path + '/old-newspaper.tsv', sep='\\t')\n",
    "# 看一下前幾筆資料是什麼樣子\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前處理\n",
    "- 訓練目標為生成繁體中文字\n",
    "    - 所以只考量繁體中文的資料\n",
    "    - 類別為 `Chinese (Traditional)`\n",
    "    - 共約 333735 筆\n",
    "- 資料長度不一\n",
    "    - 畫出長度分佈圖\n",
    "    - 計算長度四分位數、最小值、最大值\n",
    "    - 為了方便訓練，只考慮長度介於 60~200 的新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333735, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Language'] == 'Chinese (Traditional)'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只取前7000筆，因爲原資料量太大了，不方便演示\n",
    "df = df[df['Language'] == 'Chinese (Traditional)'].iloc[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    7000.000000\n",
      "mean       96.673714\n",
      "std        70.320249\n",
      "min         2.000000\n",
      "25%        43.000000\n",
      "50%        90.000000\n",
      "75%       133.000000\n",
      "max       815.000000\n",
      "Name: len, dtype: float64\n",
      "6508\n",
      "4782\n",
      "4290\n"
     ]
    }
   ],
   "source": [
    "# 簡單做一下統計\n",
    "df['len'] = df['Text'].apply(lambda x: len(str(x)))\n",
    "print(df['len'].describe())\n",
    "print(df[df['len'] <= 200].shape[0])\n",
    "print(df[df['len'] >= 60].shape[0])\n",
    "print(df[(df['len'] >= 60) & (df['len'] <= 200)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照字串長度篩選一下資料集，可做可不做，看需求\n",
    "df = df[(df['len'] >= 60) & (df['len'] <= 200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立字典\n",
    "- 無法直接利用純文字進行計算\n",
    "- 將所有文字轉換成數字\n",
    "- 字典大小約為 `7000`\n",
    "- 特殊字\n",
    "    - '&lt;pad&gt;'\n",
    "        - 每個 batch 所包含的句子長度不同\n",
    "        - 將長度使用 '&lt;pad&gt;' 補成 batch 中最大值者\n",
    "    - '&lt;eos&gt;'\n",
    "        - 指定生成的結尾\n",
    "        - 沒有 '&lt;eos&gt;' 會不知道何時停止生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小: 4363\n"
     ]
    }
   ],
   "source": [
    "# 一個dict把中文字符轉化成id\n",
    "char_to_id = {}\n",
    "# 把id轉回中文字符\n",
    "id_to_char = {}\n",
    "\n",
    "# 有一些必須要用的special token先添加進來(一般用來做padding的token的id是0)\n",
    "char_to_id['<pad>'] = 0\n",
    "char_to_id['<eos>'] = 1\n",
    "id_to_char[0] = '<pad>'\n",
    "id_to_char[1] = '<eos>'\n",
    "\n",
    "# 把所有資料集中出現的token都記錄到dict中\n",
    "for char in set(df['Text'].str.cat()):\n",
    "    ch_id = len(char_to_id)\n",
    "    char_to_id[char] = ch_id\n",
    "    id_to_char[ch_id] = char\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "print('字典大小: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>char_id_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1918193</th>\n",
       "      <td>方道生指，梁頌學的腦大靜脈由於受擠壓，之前有大約三分一的腦靜脈閉塞，問題嚴重，而菲國醫生用了...</td>\n",
       "      <td>[320, 1930, 1840, 2980, 1076, 2972, 858, 3931,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918194</th>\n",
       "      <td>關於「建材下鄉」在市場上已傳聞許久。從去年9月起，中國相關行業協會和組織就已經開始為「建材下...</td>\n",
       "      <td>[4279, 2502, 1919, 725, 1386, 4288, 682, 1664,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918195</th>\n",
       "      <td>報告說：“調查結果，令人非常失望。沒有一家企業能獲得最高的五星，近46%的企業處於零級。僅有...</td>\n",
       "      <td>[2017, 90, 296, 3827, 2852, 252, 2294, 1548, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918196</th>\n",
       "      <td>女人唔易做，美人更不易為。保養完美無瑕的外貌很倦人，剛顧及對抗臉上皺紋，又說頸紋更加暴露年紀...</td>\n",
       "      <td>[3096, 3065, 3145, 2640, 4126, 1076, 3352, 306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918197</th>\n",
       "      <td>王奇說，由於很多人擔心課堂上就是用來分組玩牌，所以他只安排了非常少的實踐內容，大部分時間是講...</td>\n",
       "      <td>[3482, 894, 296, 1076, 180, 2502, 1785, 2647, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Text  \\\n",
       "1918193  方道生指，梁頌學的腦大靜脈由於受擠壓，之前有大約三分一的腦靜脈閉塞，問題嚴重，而菲國醫生用了...   \n",
       "1918194  關於「建材下鄉」在市場上已傳聞許久。從去年9月起，中國相關行業協會和組織就已經開始為「建材下...   \n",
       "1918195  報告說：“調查結果，令人非常失望。沒有一家企業能獲得最高的五星，近46%的企業處於零級。僅有...   \n",
       "1918196  女人唔易做，美人更不易為。保養完美無瑕的外貌很倦人，剛顧及對抗臉上皺紋，又說頸紋更加暴露年紀...   \n",
       "1918197  王奇說，由於很多人擔心課堂上就是用來分組玩牌，所以他只安排了非常少的實踐內容，大部分時間是講...   \n",
       "\n",
       "                                              char_id_list  \n",
       "1918193  [320, 1930, 1840, 2980, 1076, 2972, 858, 3931,...  \n",
       "1918194  [4279, 2502, 1919, 725, 1386, 4288, 682, 1664,...  \n",
       "1918195  [2017, 90, 296, 3827, 2852, 252, 2294, 1548, 4...  \n",
       "1918196  [3096, 3065, 3145, 2640, 4126, 1076, 3352, 306...  \n",
       "1918197  [3482, 894, 296, 1076, 180, 2502, 1785, 2647, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把資料集的所有資料都變成id\n",
    "df['char_id_list'] = df['Text'].apply(lambda text: [char_to_id[char] for char in list(text)] + [char_to_id['<eos>']])\n",
    "df[['Text', 'char_id_list']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超參數\n",
    "\n",
    "|超參數|意義|數值|\n",
    "|-|-|-|\n",
    "|`batch_size`|單一 batch 的資料數|64|\n",
    "|`epochs`|總共要訓練幾個 epoch|10|\n",
    "|`embed_dim`|文字的 embedding 維度|50|\n",
    "|`hidden_dim`|LSTM 中每個時間的 hidden state 維度|50|\n",
    "|`lr`|Learning Rate|0.001|\n",
    "|`grad_clip`|為了避免 RNN 出現梯度爆炸問題，將梯度限制範圍|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "embed_dim = 256\n",
    "hidden_dim = 256\n",
    "lr = 0.001\n",
    "grad_clip = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料分批\n",
    "- 使用 `torch.utils.data.Dataset` 建立資料產生的工具 `dataset`\n",
    "- 再使用 `torch.utils.data.DataLoader` 對資料集 `dataset` 隨機抽樣並作為一個 batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這裏的dataset是文本生成的dataset，輸入和輸出的資料都是文章\n",
    "# 舉個例子，現在的狀況是：\n",
    "# input:  A B C D E F\n",
    "# output: B C D E F <eos>\n",
    "# 而對於加減法的任務：\n",
    "# input:  1 + 2 + 3 = 6\n",
    "# output: / / / / / 6 <eos>\n",
    "# /的部分都不用算loss，主要是預測=的後面，這裏的答案是6，所以output是6 <eos>\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # input:  A B C D E F \n",
    "        # output: B C D E F <eos>\n",
    "        x = self.sequences.iloc[index][:-1]\n",
    "        y = self.sequences.iloc[index][1:]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    batch_x = [torch.tensor(data[0]) for data in batch] # list[torch.tensor]\n",
    "    batch_y = [torch.tensor(data[1]) for data in batch] # list[torch.tensor]\n",
    "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
    "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
    "    \n",
    "    # torch.tensor\n",
    "    # [[1968, 1891, 3580, ... , 0, 0, 0],\n",
    "    #  [1014, 2242, 2247, ... , 0, 0, 0],\n",
    "    #  [3032,  522, 1485, ... , 0, 0, 0]]\n",
    "    #                       padding↑\n",
    "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(batch_x,\n",
    "                                                  batch_first=True, # shape=(batch_size, seq_len)\n",
    "                                                  padding_value=char_to_id['<pad>'])\n",
    "    \n",
    "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(batch_y,\n",
    "                                                  batch_first=True, # shape=(batch_size, seq_len)\n",
    "                                                  padding_value=char_to_id['<pad>'])\n",
    "    \n",
    "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(df['char_id_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型設計\n",
    "\n",
    "## 執行順序\n",
    "1. 將句子中的所有字轉換成 embedding\n",
    "2. 按照句子順序將 embedding 丟入 LSTM\n",
    "3. LSTM 的輸出再丟給 LSTM，可以接上更多層\n",
    "4. 最後的 LSTM 所有時間點的輸出丟進一層 Fully Connected\n",
    "5. 輸出結果所有維度中的最大者即為下一個字\n",
    "\n",
    "## 損失函數\n",
    "因為是類別預測，所以使用 Cross Entropy\n",
    "\n",
    "## 梯度更新\n",
    "使用 Adam 演算法進行梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        # Embedding層\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=embed_dim,\n",
    "                                            padding_idx=char_to_id['<pad>'])\n",
    "        \n",
    "        # RNN層\n",
    "        self.rnn_layer1 = torch.nn.RNN(input_size=embed_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.rnn_layer2 = torch.nn.RNN(input_size=hidden_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        # output層\n",
    "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,\n",
    "                                                          out_features=hidden_dim),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=hidden_dim,\n",
    "                                                          out_features=vocab_size))\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "    \n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        batch_x = self.embedding(batch_x)\n",
    "        \n",
    "        # 假設有個tensor : tensor([[1, 2, 3, 4],\n",
    "        #                        [9, 0, 0, 0]])\n",
    "        # 輸出就是：PackedSequence(data=tensor([1, 9, 2, 3, 4]), \n",
    "        #                         batch_sizes=tensor([2, 1, 1, 1]), \n",
    "        #                         sorted_indices=None, unsorted_indices=None)\n",
    "        # torch.nn.utils.rnn.pack_padded_sequence 會把batch當中的句子從長到短排序，建立如上所示的資料結構\n",
    "        # 就像上一個例子一樣，RNN會先吃第一個batch內的第一個batch_size，看到這個地方的batch_size爲2，所以此時RNN會吃兩個token，輸出一個2Xhidden_dim的向量組\n",
    "        # 然後看第二個batch_size, 此時爲1，少了一個，說明其中一個序列到頭了，那就取上一個輸出向量的第一個，再生成一個1Xhidden_dim的向量\n",
    "        batch_x = torch.nn.utils.rnn.pack_padded_sequence(batch_x,\n",
    "                                                          batch_x_lens,\n",
    "                                                          batch_first=True,\n",
    "                                                          enforce_sorted=False)\n",
    "        \n",
    "        batch_x, _ = self.rnn_layer1(batch_x)\n",
    "        batch_x, _ = self.rnn_layer2(batch_x)\n",
    "        \n",
    "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x,\n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        batch_x = self.linear(batch_x)\n",
    "        \n",
    "        return batch_x\n",
    "    \n",
    "    def generator(self, start_char, max_len=200):\n",
    "        \n",
    "        char_list = [char_to_id[start_char]]\n",
    "        \n",
    "        next_char = None\n",
    "        \n",
    "        # 生成的長度沒達到max_len就一直生\n",
    "        while len(char_list) < max_len: \n",
    "            x = torch.LongTensor(char_list).unsqueeze(0)\n",
    "            x = self.embedding(x)\n",
    "            _, (ht, _) = self.rnn_layer1(x)\n",
    "            _, (ht, _) = self.rnn_layer2(ht)\n",
    "            y = self.linear(ht)\n",
    "            \n",
    "            next_char = np.argmax(y.numpy())\n",
    "            \n",
    "            # 如果看到新的token是<eos>就說明生成結束了，就停下\n",
    "            if next_char == char_to_id['<eos>']:\n",
    "                break\n",
    "            \n",
    "            char_list.append(next_char)\n",
    "            \n",
    "        return [id_to_char[ch_id] for ch_id in char_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = CharRNN(vocab_size,\n",
    "                embed_dim,\n",
    "                hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'], reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練\n",
    "1. 最外層的 `for` 迴圈控制 `epoch`\n",
    "    1. 內層的 `for` 迴圈透過 `data_loader` 取得 batch\n",
    "        1. 丟給 `model` 進行訓練\n",
    "        2. 預測結果 `batch_pred_y` 跟真正的答案 `batch_y` 進行 Cross Entropy 得到誤差 `loss`\n",
    "        3. 使用 `loss.backward` 自動計算梯度\n",
    "        4. 使用 `torch.nn.utils.clip_grad_value_` 將梯度限制在 `-grad_clip` &lt; &lt; `grad_clip` 之間\n",
    "        5. 使用 `optimizer.step()` 進行更新（back propagation）\n",
    "2. 每 `1000` 個 batch 就輸出一次當前的 loss 觀察是否有收斂的趨勢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 0/68 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y, batch_x_lens, batch_y_lens \u001b[38;5;129;01min\u001b[39;00m process_bar:\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     batch_pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_x_lens)\n\u001b[1;32m     12\u001b[0m     batch_pred_y \u001b[38;5;241m=\u001b[39m batch_pred_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n\u001b[1;32m     13\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y, batch_x_lens, batch_y_lens \u001b[38;5;129;01min\u001b[39;00m process_bar:\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     batch_pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_x_lens)\n\u001b[1;32m     12\u001b[0m     batch_pred_y \u001b[38;5;241m=\u001b[39m batch_pred_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n\u001b[1;32m     13\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3917/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3917/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "i = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    process_bar = tqdm(data_loader, desc=f\"Training epoch {epoch}\")\n",
    "    for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
    "        \n",
    "        # 標準DL訓練幾板斧\n",
    "        optimizer.zero_grad()\n",
    "        batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
    "        batch_pred_y = batch_pred_y.view(-1, vocab_size)\n",
    "        batch_y = batch_y.view(-1).to(device)\n",
    "        loss = criterion(batch_pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        if i%10==0:\n",
    "            process_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # 麻煩各位同學加上 validation 的部分\n",
    "    # validation_process_bar = tqdm(...)\n",
    "    # for ... in validation_process_bar:\n",
    "    #     pred = model..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
